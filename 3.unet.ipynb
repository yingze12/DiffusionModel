{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0bdca1b",
   "metadata": {},
   "source": [
    "残差连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c904309f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 640, 32, 32])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class Resnet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "\n",
    "        self.time = torch.nn.Sequential(\n",
    "            torch.nn.SiLU(),\n",
    "            torch.torch.nn.Linear(1280, dim_out),\n",
    "            torch.nn.Unflatten(dim=1, unflattened_size=(dim_out, 1, 1)),\n",
    "        )\n",
    "\n",
    "        self.s0 = torch.nn.Sequential(\n",
    "            torch.torch.nn.GroupNorm(num_groups=32,\n",
    "                                     num_channels=dim_in,\n",
    "                                     eps=1e-05,\n",
    "                                     affine=True),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.torch.nn.Conv2d(dim_in,\n",
    "                                  dim_out,\n",
    "                                  kernel_size=3,\n",
    "                                  stride=1,\n",
    "                                  padding=1),\n",
    "        )\n",
    "\n",
    "        self.s1 = torch.nn.Sequential(\n",
    "            torch.torch.nn.GroupNorm(num_groups=32,\n",
    "                                     num_channels=dim_out,\n",
    "                                     eps=1e-05,\n",
    "                                     affine=True),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.torch.nn.Conv2d(dim_out,\n",
    "                                  dim_out,\n",
    "                                  kernel_size=3,\n",
    "                                  stride=1,\n",
    "                                  padding=1),\n",
    "        )\n",
    "\n",
    "        self.res = None\n",
    "        if dim_in != dim_out:\n",
    "            self.res = torch.torch.nn.Conv2d(dim_in,\n",
    "                                             dim_out,\n",
    "                                             kernel_size=1,\n",
    "                                             stride=1,\n",
    "                                             padding=0)\n",
    "\n",
    "    def forward(self, x, time):\n",
    "        #x -> [1, 320, 32, 32]\n",
    "        #time -> [1, 1280]\n",
    "\n",
    "        res = x\n",
    "\n",
    "        #[1, 1280] -> [1, 640, 1, 1]\n",
    "        time = self.time(time)\n",
    "\n",
    "        #[1, 320, 32, 32] -> [1, 640, 32, 32]\n",
    "        x = self.s0(x) + time\n",
    "\n",
    "        #维度不变\n",
    "        #[1, 640, 32, 32]\n",
    "        x = self.s1(x)\n",
    "\n",
    "        #[1, 320, 64, 64] -> [1, 640, 32, 32]\n",
    "        if self.res:\n",
    "            res = self.res(res)\n",
    "\n",
    "        #维度不变\n",
    "        #[1, 640, 32, 32]\n",
    "        x = res + x\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "Resnet(320, 640)(torch.randn(1, 320, 32, 32), torch.randn(1, 1280)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0ea0b5",
   "metadata": {},
   "source": [
    "UNet的注意力层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "befaba98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4096, 320])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CrossAttention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    初始化部分\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_q, dim_kv):\n",
    "        #dim_q -> 320\n",
    "        #dim_kv -> 768\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim_q = dim_q\n",
    "\n",
    "        self.q = torch.nn.Linear(dim_q, dim_q, bias=False)\n",
    "        self.k = torch.nn.Linear(dim_kv, dim_q, bias=False)\n",
    "        self.v = torch.nn.Linear(dim_kv, dim_q, bias=False)\n",
    "\n",
    "        self.out = torch.nn.Linear(dim_q, dim_q)\n",
    "\n",
    "    \"\"\"\n",
    "    图文注意力\n",
    "    计算过程\n",
    "    无mask\n",
    "    多头注意力\n",
    "    q是图像数据，kv是文本数据，q，kv不相等，计算的是交叉注意力\n",
    "    \"\"\"\n",
    "    def forward(self, q, kv):\n",
    "        #q -> [1, 4096, 320]\n",
    "        #kv -> [1, 77, 768]\n",
    "\n",
    "        #[1, 4096, 320] -> [1, 4096, 320]\n",
    "        q = self.q(q)\n",
    "        #[1, 77, 768] -> [1, 77, 320]\n",
    "        k = self.k(kv)\n",
    "        #[1, 77, 768] -> [1, 77, 320]\n",
    "        v = self.v(kv)\n",
    "\n",
    "        def reshape(x):\n",
    "            #x -> [1, 4096, 320]\n",
    "            b, lens, dim = x.shape\n",
    "\n",
    "            #[1, 4096, 320] -> [1, 4096, 8, 40]\n",
    "            x = x.reshape(b, lens, 8, dim // 8)\n",
    "\n",
    "            #[1, 4096, 8, 40] -> [1, 8, 4096, 40]\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "            #[1, 8, 4096, 40] -> [8, 4096, 40]\n",
    "            x = x.reshape(b * 8, lens, dim // 8)\n",
    "\n",
    "            return x\n",
    "\n",
    "        #[1, 4096, 320] -> [8, 4096, 40]\n",
    "        q = reshape(q)\n",
    "        #[1, 77, 320] -> [8, 77, 40]\n",
    "        k = reshape(k)\n",
    "        #[1, 77, 320] -> [8, 77, 40]\n",
    "        v = reshape(v)\n",
    "\n",
    "        #[8, 4096, 40] * [8, 40, 77] -> [8, 4096, 77]\n",
    "        #atten = q.bmm(k.transpose(1, 2)) * (self.dim_q // 8)**-0.5\n",
    "\n",
    "        #从数学上是等价的,但是在实际计算时会产生很小的误差\n",
    "        atten = torch.baddbmm(\n",
    "            torch.empty(q.shape[0], q.shape[1], k.shape[1], device=q.device),\n",
    "            q,\n",
    "            k.transpose(1, 2),\n",
    "            beta=0,\n",
    "            alpha=(self.dim_q // 8)**-0.5,\n",
    "        )\n",
    "\n",
    "        atten = atten.softmax(dim=-1)\n",
    "\n",
    "        #[8, 4096, 77] * [8, 77, 40] -> [8, 4096, 40]\n",
    "        atten = atten.bmm(v)\n",
    "\n",
    "        def reshape(x):\n",
    "            #x -> [8, 4096, 40]\n",
    "            b, lens, dim = x.shape\n",
    "\n",
    "            #[8, 4096, 40] -> [1, 8, 4096, 40]\n",
    "            x = x.reshape(b // 8, 8, lens, dim)\n",
    "\n",
    "            #[1, 8, 4096, 40] -> [1, 4096, 8, 40]\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "            #[1, 4096, 320]\n",
    "            x = x.reshape(b // 8, lens, dim * 8)\n",
    "\n",
    "            return x\n",
    "\n",
    "        #[8, 4096, 40] -> [1, 4096, 320]\n",
    "        atten = reshape(atten)\n",
    "\n",
    "        #[1, 4096, 320] -> [1, 4096, 320]\n",
    "        atten = self.out(atten)\n",
    "\n",
    "        return atten\n",
    "\n",
    "\n",
    "CrossAttention(320, 768)(torch.randn(1, 4096, 320), torch.randn(1, 77,\n",
    "                                                                768)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0005ac25",
   "metadata": {},
   "source": [
    "Transformer层\n",
    "\n",
    "计算过程\n",
    "\n",
    "这是一个 U-Net/扩散模型里常见的 Transformer 块（很多人称它为 Transformer ResBlock 或 Spatial Transformer）：\n",
    "- In：对 [B, C, H, W] 归一化+1×1卷积 → 拉平成 [B, HW, C] 序列；\n",
    "- Self-Attn：图像 token 彼此交互；\n",
    "- Cross-Attn：图像 token 与文本 token 交互（多模态对齐，常见于文生图、条件扩散、开放词汇分割）；\n",
    "- MLP(GEGLU)：非线性前馈增强表达；\n",
    "- Out：把序列折回 2D，并与输入特征残差融合。\n",
    "全流程均为 Pre-Norm + 残差 设计，训练更稳定、梯度更顺畅。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d79e8415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 320, 64, 64])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dim): # dim 是通道/嵌入维度\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "\n",
    "        \"\"\"\n",
    "        In 分支：把 2D 特征变成 token 序列前的归一化+1×1卷积\n",
    "        \"\"\"\n",
    "        #in\n",
    "        self.norm_in = torch.nn.GroupNorm(num_groups=32,\n",
    "                                          num_channels=dim,\n",
    "                                          eps=1e-6,\n",
    "                                          affine=True)\n",
    "        self.cnn_in = torch.nn.Conv2d(dim,\n",
    "                                      dim,\n",
    "                                      kernel_size=1,\n",
    "                                      stride=1,\n",
    "                                      padding=0)\n",
    "\n",
    "        \"\"\"\n",
    "        注意力块：先自注意力（图像内部），再跨注意力（图像对文本）\n",
    "        atten1：q 与 kv 都来自同一图像 token 序列（自注意力）。\n",
    "        atten2：q 来自图像，kv 是外部文本特征（77 个 token，维度 768），做跨模态对齐。\n",
    "        \"\"\"\n",
    "        #atten\n",
    "        self.norm_atten0 = torch.nn.LayerNorm(dim, elementwise_affine=True)\n",
    "        self.atten1 = CrossAttention(dim, dim)\n",
    "        self.norm_atten1 = torch.nn.LayerNorm(dim, elementwise_affine=True)\n",
    "        self.atten2 = CrossAttention(dim, 768)\n",
    "\n",
    "        \"\"\"\n",
    "        激活/MLP（门控前馈 GEGLU 风格）\n",
    "        \"\"\"\n",
    "        #act\n",
    "        self.norm_act = torch.nn.LayerNorm(dim, elementwise_affine=True)\n",
    "        self.fc0 = torch.nn.Linear(dim, dim * 8)\n",
    "        self.act = torch.nn.GELU()\n",
    "        self.fc1 = torch.nn.Linear(dim * 4, dim)\n",
    "\n",
    "        \"\"\"\n",
    "        Out 分支：把 token 序列还原回 2D，再做 1×1 卷积并加上输入残差\n",
    "        \"\"\"\n",
    "        #out\n",
    "        self.cnn_out = torch.nn.Conv2d(dim,\n",
    "                                       dim,\n",
    "                                       kernel_size=1,\n",
    "                                       stride=1,\n",
    "                                       padding=0)\n",
    "\n",
    "    def forward(self, q, kv):\n",
    "        #q -> [1, 320, 64, 64]\n",
    "        #kv -> [1, 77, 768]\n",
    "        b, _, h, w = q.shape\n",
    "        res1 = q\n",
    "\n",
    "        #----in----\n",
    "        #维度不变\n",
    "        #[1, 320, 64, 64]\n",
    "        q = self.cnn_in(self.norm_in(q))\n",
    "\n",
    "        #[1, 320, 64, 64] -> [1, 64, 64, 320] -> [1, 4096, 320]\n",
    "        q = q.permute(0, 2, 3, 1).reshape(b, h * w, self.dim)\n",
    "\n",
    "        #----atten----\n",
    "        #维度不变\n",
    "        #[1, 4096, 320]\n",
    "        \"\"\"\n",
    "        1) 自注意力（图像内部）\n",
    "        先 LayerNorm，再把同一个归一化后的 q 同时作为 q 和 kv 喂给 atten1（self-attn）。\n",
    "        残差连接：输出 + q。这一步让图像 token 之间互相“沟通”。\n",
    "        \"\"\"\n",
    "        q = self.atten1(q=self.norm_atten0(q), kv=self.norm_atten0(q)) + q\n",
    "        \"\"\"\n",
    "        2) 跨注意力（图像对文本）\n",
    "        再次 LayerNorm 后，把图像 token 作为查询，文本 token（[B, 77, 768]）作为键值做 cross-attn。\n",
    "        CrossAttention(dim, 768) 内部会把文本 kv 投影到 dim=320 再与 q 匹配。\n",
    "        残差相加：跨模态融合信息（图像特征可“对齐/检索”文本概念）。\n",
    "        \"\"\"\n",
    "        q = self.atten2(q=self.norm_atten1(q), kv=kv) + q\n",
    "\n",
    "        #----act----\n",
    "        \"\"\"\n",
    "        fc0 把每个 token 维度从 320 放大到 2560 (= 8*dim)。\n",
    "        对半切分为 x1, x2（各 1280），做 x1 * GELU(x2) → 门控激活（GEGLU 思想）。\n",
    "        fc1 把 1280 压回 320，并与 res2 残差相加。\n",
    "        （这一段相当于 Transformer 里的 FFN，只是用了更强的门控变体。）\n",
    "        \"\"\"\n",
    "        #[1, 4096, 320]\n",
    "        res2 = q\n",
    "\n",
    "        #[1, 4096, 320] -> [1, 4096, 2560]\n",
    "        q = self.fc0(self.norm_act(q))\n",
    "\n",
    "        #1280\n",
    "        d = q.shape[2] // 2\n",
    "\n",
    "        #[1, 4096, 1280] * [1, 4096, 1280] -> [1, 4096, 1280]\n",
    "        q = q[:, :, :d] * self.act(q[:, :, d:])\n",
    "\n",
    "        #[1, 4096, 1280] -> [1, 4096, 320]\n",
    "        q = self.fc1(q) + res2\n",
    "\n",
    "        #----out----\n",
    "        #[1, 4096, 320] -> [1, 64, 64, 320] -> [1, 320, 64, 64]\n",
    "        q = q.reshape(b, h, w, self.dim).permute(0, 3, 1, 2).contiguous() # 把 token 序列重新“折叠”回特征图。\n",
    "\n",
    "        \"\"\"\n",
    "        1×1 Conv 再做一次通道融合，然后与最初输入的 2D 特征做残差加和（res1）。\n",
    "        返回形状与输入完全一致：[1, 320, 64, 64]。\n",
    "        \"\"\"\n",
    "        #维度不变\n",
    "        #[1, 320, 64, 64]\n",
    "        q = self.cnn_out(q) + res1\n",
    "\n",
    "        return q\n",
    "\n",
    "\n",
    "Transformer(320)(torch.randn(1, 320, 64, 64), torch.randn(1, 77, 768)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36c4fb1",
   "metadata": {},
   "source": [
    "Down层\n",
    "\n",
    "这个 DownBlock 在网络里的角色\n",
    "\n",
    "通道升维 + 条件注入（time）：让更深层有更强表征，并把扩散时间步条件灌进来。\n",
    "\n",
    "两次 (Res→TF)：在同一分辨率下先充分空间建模 + 文本对齐，避免信息还没融合就被下采样丢细节。\n",
    "\n",
    "下采样：把空间分辨率减半，进入更抽象的层级，同时把多尺度特征存入 outs 供解码端跳连。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "428962c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 640, 16, 16])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DownBlock(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tf0 = Transformer(dim_out)\n",
    "        self.res0 = Resnet(dim_in, dim_out)\n",
    "\n",
    "        self.tf1 = Transformer(dim_out)\n",
    "        self.res1 = Resnet(dim_out, dim_out)\n",
    "\n",
    "        self.out = torch.nn.Conv2d(dim_out,\n",
    "                                   dim_out,\n",
    "                                   kernel_size=3,\n",
    "                                   stride=2,\n",
    "                                   padding=1)\n",
    "\n",
    "    \"\"\"\n",
    "    输入：\n",
    "    out_vae: 图像特征 [1, 320, 32, 32]\n",
    "    out_encoder: 文本/编码器特征 [1, 77, 768]\n",
    "    time: 时间嵌入 [1, 1280]（常见做法：先经 MLP 映射，再注入到 ResNet 内）\n",
    "    \"\"\"\n",
    "    def forward(self, out_vae, out_encoder, time):\n",
    "        outs = [] # 准备收集本层的跳连特征（给 UNet 上采样端用）。\n",
    "\n",
    "        \"\"\"\n",
    "        保存第一个跳连：outs[0] = [1, 320, 32, 32] → [1, 640, 32, 32]\n",
    "        \"\"\"\n",
    "        out_vae = self.res0(out_vae, time) # 通道升维 + 融合 time 条件（如 FiLM/加性/缩放偏置等）。\n",
    "        out_vae = self.tf0(out_vae, out_encoder) # 先自注意力建模空间 token 间关系，再跨注意力用 out_encoder（77×768）为键值增强语义（图像对文本对齐/检索）。\n",
    "        outs.append(out_vae) # 保存第一个跳连：outs[0] = [1, 640, 32, 32]\n",
    "\n",
    "        out_vae = self.res1(out_vae, time) # 同样融合 time，但不改通道数。\n",
    "        out_vae = self.tf1(out_vae, out_encoder) # 再做一轮自/跨注意力，加强多模态语义与空间关系。\n",
    "        outs.append(out_vae) # 保存第二个跳连：outs[1] = [1, 640, 32, 32]\n",
    "\n",
    "        out_vae = self.out(out_vae)\n",
    "        \"\"\"\n",
    "        这是 Conv2d(640→640, k3, s2, p1)：\n",
    "        形状：[1, 640, 32, 32] → [1, 640, 16, 16]（H、W 各减半）\n",
    "        作用：进入更低分辨率层级，同时保持通道数。\n",
    "        \"\"\"\n",
    "        outs.append(out_vae)\n",
    "        \"\"\"\n",
    "        保存第三个特征：outs[2] = [1, 640, 16, 16]\n",
    "        许多实现里会把下采样前和/或下采样后的特征都存起来，方便解码端不同分辨率的 skip 使用。你这里三处都存了。\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        out_vae：下采样后的输出，形状 [1, 640, 16, 16]（你在代码末尾打印的就是这个）\n",
    "        outs：长度为 3 的列表：\n",
    "        outs[0] = [1, 640, 32, 32]（第一组 Res+TF 之后）\n",
    "        outs[1] = [1, 640, 32, 32]（第二组 Res+TF 之后）\n",
    "        outs[2] = [1, 640, 16, 16]（下采样卷积之后）\n",
    "        \"\"\"\n",
    "        return out_vae, outs\n",
    "\n",
    "\n",
    "DownBlock(320, 640)(torch.randn(1, 320, 32, 32), torch.randn(1, 77, 768),\n",
    "                    torch.randn(1, 1280))[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6be2334",
   "metadata": {},
   "source": [
    "Up层\n",
    "\n",
    "带跨层连接\n",
    "\n",
    "三次“拼接→ResNet→Transformer”：\n",
    "逐步融合来自编码端/下采样端的三路 skip 特征（两路 640 通道 + 一路 320 通道），每次拼接后用 ResNet 把通道压回到统一的 640，再用 Transformer 与 out_encoder（如文本）做跨模态交互（cross-attn），增强语义对齐。\n",
    "\n",
    "pop() 顺序：\n",
    "从尾部拿，保证与下采样端对称（编码时 append 的最后一个，解码时最先用），分辨率上这三路都和当前 out_vae 对齐（都是 32×32），因此能直接 cat。\n",
    "\n",
    "time 向量 [1,1280]：\n",
    "残差块内部通常会把时间嵌入（或噪声/步数嵌入）投影到通道维度后加到特征里；因此 time 的原始长度和输出通道数不同没关系，模块内部会线性映射。\n",
    "\n",
    "上采样位置：\n",
    "UpBlock 的语义是“解码/上采样”阶段的一个层级。设置 add_up=True 时，这个块负责把空间尺度放大一倍，为更高分辨率的下一层做准备。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d97e041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 640, 64, 64])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class UpBlock(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dim_in, dim_out, dim_prev, add_up): # 这里 dim_in=320, dim_out=640, dim_prev=1280, add_up=True。\n",
    "        super().__init__()\n",
    "\n",
    "        self.res0 = Resnet(dim_out + dim_prev, dim_out) # 第一个残差块（ResNet block），输入通道数是 dim_out + dim_prev = 640 + 1280 = 1920，输出通道数是 dim_out = 640。\n",
    "                                                        # 用意：先把当前主支路(out_vae，通道 1280)与第1个跳连特征拼接后（通道加起来），再压回到 640 通道。\n",
    "        self.res1 = Resnet(dim_out + dim_out, dim_out) # 第二个残差块，输入 640 + 640 = 1280，输出 640。\n",
    "                                                       # 用意：再和第二个跳连特征（640 通道）拼接→压回 640。\n",
    "        self.res2 = Resnet(dim_in + dim_out, dim_out) # 第三个残差块，输入 dim_in + dim_out = 320 + 640 = 960，输出 640。\n",
    "                                                      # 用意：最后和第三个跳连特征（320 通道）拼接→压回 640。\n",
    "\n",
    "        \"\"\"\n",
    "        三个 Transformer（通常是跨注意力 cross-attn 到 out_encoder 文本/编码器特征），通道不变，均保持 640。\n",
    "        \"\"\"\n",
    "        self.tf0 = Transformer(dim_out)\n",
    "        self.tf1 = Transformer(dim_out)\n",
    "        self.tf2 = Transformer(dim_out)\n",
    "\n",
    "        \"\"\"\n",
    "        若 add_up=True，最后会上采样 x2（nearest）到更大的空间分辨率，然后再一个 3×3 卷积（通道数仍 640）。\n",
    "        \"\"\"\n",
    "        self.out = None\n",
    "        if add_up:\n",
    "            self.out = torch.nn.Sequential(\n",
    "                torch.nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "                torch.nn.Conv2d(dim_out, dim_out, kernel_size=3, padding=1),\n",
    "            )\n",
    "\n",
    "    def forward(self, out_vae, out_encoder, time, out_down):\n",
    "        out_vae = self.res0(torch.cat([out_vae, out_down.pop()], dim=1), time) # list.pop() 是从末尾弹出，因此 3 次 pop() 的顺序依次得到\n",
    "        \"\"\"\n",
    "        先拼接: out_vae [1,1280,32,32] 与 pop()[1,640,32,32] → [1, 1920, 32, 32]\n",
    "        过 Resnet(1920→640) → out_vae [1, 640, 32, 32]\n",
    "        \"\"\"\n",
    "        out_vae = self.tf0(out_vae, out_encoder) # Transformer 保持通道不变 → [1, 640, 32, 32]\n",
    "\n",
    "        out_vae = self.res1(torch.cat([out_vae, out_down.pop()], dim=1), time)\n",
    "        \"\"\"\n",
    "        # 拼接: [1,640,32,32] 与 pop()[1,640,32,32] → [1, 1280, 32, 32]\n",
    "        # Resnet(1280→640) → [1, 640, 32, 32]\n",
    "        \"\"\"\n",
    "        out_vae = self.tf1(out_vae, out_encoder) # Transformer 保持不变 → [1, 640, 32, 32]\n",
    "\n",
    "        out_vae = self.res2(torch.cat([out_vae, out_down.pop()], dim=1), time)\n",
    "        \"\"\"\n",
    "        # 拼接: [1,640,32,32] 与 pop()[1,320,32,32] → [1, 960, 32, 32]\n",
    "        # Resnet(960→640) → [1, 640, 32, 32]\n",
    "        \"\"\"\n",
    "        out_vae = self.tf2(out_vae, out_encoder) # Transformer 保持不变 → [1, 640, 32, 32]\n",
    "\n",
    "        if self.out:\n",
    "            out_vae = self.out(out_vae)\n",
    "        # Upsample(scale_factor=2): [1, 640, 32, 32] → [1, 640, 64, 64]\n",
    "        # Conv3x3(640→640, padding=1): 形状不变 → [1, 640, 64, 64]\n",
    "\n",
    "        return out_vae\n",
    "\n",
    "\n",
    "UpBlock(320, 640, 1280, True)(torch.randn(1, 1280, 32, 32),\n",
    "                              torch.randn(1, 77, 768), torch.randn(1, 1280), [\n",
    "                                  torch.randn(1, 320, 32, 32),\n",
    "                                  torch.randn(1, 640, 32, 32),\n",
    "                                  torch.randn(1, 640, 32, 32)\n",
    "                              ]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf5b106",
   "metadata": {},
   "source": [
    "整体结构速览（UNet 做了什么）\n",
    "- 输入侧（in）：\n",
    "  - in_vae: Conv2d(4→320) 把 [B,4,64,64] 提升到 [B,320,64,64]\n",
    "  - in_time: Linear(320→1280→1280) 把时间嵌入弄到通道 1280\n",
    "- 下采样（down）：三个 DownBlock\n",
    "    - DownBlock(320→320)：输出 [B,320,32,32]\n",
    "    - DownBlock(320→640)：输出 [B,640,16,16]\n",
    "    - DownBlock(640→1280)：输出 [B,1280,8,8]\n",
    "    - 额外两层 Resnet(1280→1280)（都在 8×8），并且把中间特征一路 append 到 out_down，供后面 U 形结构的跳连使用\n",
    "- 中间（mid）：\n",
    "  - Resnet(1280) → Transformer(1280) → Resnet(1280)，都在 8×8\n",
    "- 上采样（up）：\n",
    "  - 三个先拼接再 Resnet的块：up_res0/up_res1/up_res2，每次都把当前特征 [B,1280,8,8] 与 out_down.pop()（也是 [B,1280,8,8]）拼，一起过 Resnet(2560→1280)\n",
    "  - up_in 上采样 8→16，保持通道 1280\n",
    "  - 三个 UpBlock（每个内部都是三次“拼接→Resnet→Transformer”，并按 add_up 决定是否再上采样）：\n",
    "    - UpBlock(640, 1280, 1280, True)：输出 [B,1280,32,32]\n",
    "    - UpBlock(320, 640, 1280, True)：输出 [B,640,64,64]\n",
    "    - UpBlock(320, 320, 640, False)：输出 [B,320,64,64]\n",
    "- 输出侧（out）：\n",
    "  - GroupNorm(320, groups=32) + SiLU + Conv2d(320→4)：最终 [B,4,64,64]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b94cacd",
   "metadata": {},
   "source": [
    "UNet模型\n",
    "\n",
    "初始化部分\n",
    "\n",
    "forward函数\n",
    "1层in （卷积特征图 编码time）\n",
    "\n",
    "4层down （计算图文注意力 一层一层计算 保留全部计算结果）\n",
    "\n",
    "1层middle （计算图文注意力）\n",
    "\n",
    "4层up （跨层链接down 一层一层计算）\n",
    "\n",
    "1层out （卷积后输出）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdae0828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 64, 64])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class UNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #in\n",
    "        self.in_vae = torch.nn.Conv2d(4, 320, kernel_size=3, padding=1)\n",
    "\n",
    "        self.in_time = torch.nn.Sequential(\n",
    "            torch.nn.Linear(320, 1280),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(1280, 1280),\n",
    "        )\n",
    "\n",
    "        #down\n",
    "        self.down_block0 = DownBlock(320, 320)\n",
    "        self.down_block1 = DownBlock(320, 640)\n",
    "        self.down_block2 = DownBlock(640, 1280)\n",
    "\n",
    "        self.down_res0 = Resnet(1280, 1280)\n",
    "        self.down_res1 = Resnet(1280, 1280)\n",
    "\n",
    "        #mid\n",
    "        self.mid_res0 = Resnet(1280, 1280)\n",
    "        self.mid_tf = Transformer(1280)\n",
    "        self.mid_res1 = Resnet(1280, 1280)\n",
    "\n",
    "        #up\n",
    "        self.up_res0 = Resnet(2560, 1280)\n",
    "        self.up_res1 = Resnet(2560, 1280)\n",
    "        self.up_res2 = Resnet(2560, 1280)\n",
    "\n",
    "        self.up_in = torch.nn.Sequential(\n",
    "            torch.nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            torch.nn.Conv2d(1280, 1280, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "        self.up_block0 = UpBlock(640, 1280, 1280, True)\n",
    "        self.up_block1 = UpBlock(320, 640, 1280, True)\n",
    "        self.up_block2 = UpBlock(320, 320, 640, False)\n",
    "\n",
    "        #out\n",
    "        self.out = torch.nn.Sequential(\n",
    "            torch.nn.GroupNorm(num_channels=320, num_groups=32, eps=1e-5),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Conv2d(320, 4, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, out_vae, out_encoder, time):\n",
    "        #out_vae -> [1, 4, 64, 64]\n",
    "        #out_encoder -> [1, 77, 768]\n",
    "        #time -> [1]\n",
    "\n",
    "        #----in----\n",
    "        #[1, 4, 64, 64] -> [1, 320, 64, 64]\n",
    "        out_vae = self.in_vae(out_vae)\n",
    "\n",
    "        def get_time_embed(t):\n",
    "            #-9.210340371976184 = -math.log(10000)\n",
    "            e = torch.arange(160) * -9.210340371976184 / 160\n",
    "            e = e.exp().to(t.device) * t\n",
    "\n",
    "            #[160+160] -> [320] -> [1, 320]\n",
    "            e = torch.cat([e.cos(), e.sin()]).unsqueeze(dim=0)\n",
    "\n",
    "            return e\n",
    "\n",
    "        #[1] -> [1, 320]\n",
    "        time = get_time_embed(time)\n",
    "        #[1, 320] -> [1, 1280]\n",
    "        time = self.in_time(time)\n",
    "\n",
    "        #----down----\n",
    "        #[1, 320, 64, 64]\n",
    "        #[1, 320, 64, 64]\n",
    "        #[1, 320, 64, 64]\n",
    "        #[1, 320, 32, 32]\n",
    "        #[1, 640, 32, 32]\n",
    "        #[1, 640, 32, 32]\n",
    "        #[1, 640, 16, 16]\n",
    "        #[1, 1280, 16, 16]\n",
    "        #[1, 1280, 16, 16]\n",
    "        #[1, 1280, 8, 8]\n",
    "        #[1, 1280, 8, 8]\n",
    "        #[1, 1280, 8, 8]\n",
    "        out_down = [out_vae]\n",
    "\n",
    "        #[1, 320, 64, 64],[1, 77, 768],[1, 1280] -> [1, 320, 32, 32]\n",
    "        #out -> [1, 320, 64, 64],[1, 320, 64, 64][1, 320, 32, 32]\n",
    "        out_vae, out = self.down_block0(out_vae=out_vae,\n",
    "                                        out_encoder=out_encoder,\n",
    "                                        time=time)\n",
    "        out_down.extend(out)\n",
    "\n",
    "        #[1, 320, 32, 32],[1, 77, 768],[1, 1280] -> [1, 640, 16, 16]\n",
    "        #out -> [1, 640, 32, 32],[1, 640, 32, 32],[1, 640, 16, 16]\n",
    "        out_vae, out = self.down_block1(out_vae=out_vae,\n",
    "                                        out_encoder=out_encoder,\n",
    "                                        time=time)\n",
    "        out_down.extend(out)\n",
    "\n",
    "        #[1, 640, 16, 16],[1, 77, 768],[1, 1280] -> [1, 1280, 8, 8]\n",
    "        #out -> [1, 1280, 16, 16],[1, 1280, 16, 16],[1, 1280, 8, 8]\n",
    "        out_vae, out = self.down_block2(out_vae=out_vae,\n",
    "                                        out_encoder=out_encoder,\n",
    "                                        time=time)\n",
    "        out_down.extend(out)\n",
    "\n",
    "        #[1, 1280, 8, 8],[1, 1280] -> [1, 1280, 8, 8]\n",
    "        out_vae = self.down_res0(out_vae, time)\n",
    "        out_down.append(out_vae)\n",
    "\n",
    "        #[1, 1280, 8, 8],[1, 1280] -> [1, 1280, 8, 8]\n",
    "        out_vae = self.down_res1(out_vae, time)\n",
    "        out_down.append(out_vae)\n",
    "\n",
    "        #----mid----\n",
    "        #[1, 1280, 8, 8],[1, 1280] -> [1, 1280, 8, 8]\n",
    "        out_vae = self.mid_res0(out_vae, time)\n",
    "\n",
    "        #[1, 1280, 8, 8],[1, 77, 768] -> [1, 1280, 8, 8]\n",
    "        out_vae = self.mid_tf(out_vae, out_encoder)\n",
    "\n",
    "        #[1, 1280, 8, 8],[1, 1280] -> [1, 1280, 8, 8]\n",
    "        out_vae = self.mid_res1(out_vae, time)\n",
    "\n",
    "        #----up----\n",
    "        #[1, 1280+1280, 8, 8],[1, 1280] -> [1, 1280, 8, 8]\n",
    "        out_vae = self.up_res0(torch.cat([out_vae, out_down.pop()], dim=1),\n",
    "                               time)\n",
    "\n",
    "        #[1, 1280+1280, 8, 8],[1, 1280] -> [1, 1280, 8, 8]\n",
    "        out_vae = self.up_res1(torch.cat([out_vae, out_down.pop()], dim=1),\n",
    "                               time)\n",
    "\n",
    "        #[1, 1280+1280, 8, 8],[1, 1280] -> [1, 1280, 8, 8]\n",
    "        out_vae = self.up_res2(torch.cat([out_vae, out_down.pop()], dim=1),\n",
    "                               time)\n",
    "\n",
    "        #[1, 1280, 8, 8] -> [1, 1280, 16, 16]\n",
    "        out_vae = self.up_in(out_vae)\n",
    "\n",
    "        #[1, 1280, 16, 16],[1, 77, 768],[1, 1280] -> [1, 1280, 32, 32]\n",
    "        #out_down -> [1, 640, 16, 16],[1, 1280, 16, 16],[1, 1280, 16, 16]\n",
    "        out_vae = self.up_block0(out_vae=out_vae,\n",
    "                                 out_encoder=out_encoder,\n",
    "                                 time=time,\n",
    "                                 out_down=out_down)\n",
    "\n",
    "        #[1, 1280, 32, 32],[1, 77, 768],[1, 1280] -> [1, 640, 64, 64]\n",
    "        #out_down -> [1, 320, 32, 32],[1, 640, 32, 32],[1, 640, 32, 32]\n",
    "        out_vae = self.up_block1(out_vae=out_vae,\n",
    "                                 out_encoder=out_encoder,\n",
    "                                 time=time,\n",
    "                                 out_down=out_down)\n",
    "\n",
    "        #[1, 640, 64, 64],[1, 77, 768],[1, 1280] -> [1, 320, 64, 64]\n",
    "        #out_down -> [1, 320, 64, 64],[1, 320, 64, 64],[1, 320, 64, 64]\n",
    "        out_vae = self.up_block2(out_vae=out_vae,\n",
    "                                 out_encoder=out_encoder,\n",
    "                                 time=time,\n",
    "                                 out_down=out_down)\n",
    "\n",
    "        #----out----\n",
    "        #[1, 320, 64, 64] -> [1, 4, 64, 64]\n",
    "        out_vae = self.out(out_vae)\n",
    "\n",
    "        return out_vae\n",
    "\n",
    "\n",
    "UNet()(torch.randn(2, 4, 64, 64), torch.randn(2, 77, 768),\n",
    "    torch.LongTensor([26])).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3f23c7",
   "metadata": {},
   "source": [
    "加载预训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45d140d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5043d8cae3e04b598fb49a3b58bcbdf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UNet2DConditionModel\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#加载预训练模型的参数\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43mUNet2DConditionModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlansinuote/diffsion_from_scratch.params\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43munet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m unet \u001b[38;5;241m=\u001b[39m UNet()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#in\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torch_env\\lib\\site-packages\\diffusers\\models\\modeling_utils.py:506\u001b[0m, in \u001b[0;36mModelMixin.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 506\u001b[0m     model_file \u001b[38;5;241m=\u001b[39m \u001b[43m_get_model_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mWEIGHTS_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage:\n\u001b[0;32m    521\u001b[0m     \u001b[38;5;66;03m# Instantiate model with empty weights\u001b[39;00m\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m accelerate\u001b[38;5;241m.\u001b[39minit_empty_weights():\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torch_env\\lib\\site-packages\\diffusers\\models\\modeling_utils.py:797\u001b[0m, in \u001b[0;36m_get_model_file\u001b[1;34m(pretrained_model_name_or_path, weights_name, subfolder, cache_dir, force_download, proxies, resume_download, local_files_only, use_auth_token, user_agent, revision)\u001b[0m\n\u001b[0;32m    794\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    795\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    796\u001b[0m         \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 797\u001b[0m         model_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    806\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    807\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    810\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m model_file\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torch_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1226\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, use_auth_token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1223\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m temp_file_manager() \u001b[38;5;28;01mas\u001b[39;00m temp_file:\n\u001b[0;32m   1224\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdownloading \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, temp_file\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m-> 1226\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1232\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1234\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstoring \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in cache at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, blob_path)\n\u001b[0;32m   1235\u001b[0m os\u001b[38;5;241m.\u001b[39mreplace(temp_file\u001b[38;5;241m.\u001b[39mname, blob_path)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torch_env\\lib\\site-packages\\huggingface_hub\\file_download.py:490\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries)\u001b[0m\n\u001b[0;32m    481\u001b[0m total \u001b[38;5;241m=\u001b[39m resume_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(content_length) \u001b[38;5;28;01mif\u001b[39;00m content_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    482\u001b[0m progress \u001b[38;5;241m=\u001b[39m tqdm(\n\u001b[0;32m    483\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    484\u001b[0m     unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    488\u001b[0m     disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(logger\u001b[38;5;241m.\u001b[39mgetEffectiveLevel() \u001b[38;5;241m==\u001b[39m logging\u001b[38;5;241m.\u001b[39mNOTSET),\n\u001b[0;32m    489\u001b[0m )\n\u001b[1;32m--> 490\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m):\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    492\u001b[0m         progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torch_env\\lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torch_env\\lib\\site-packages\\urllib3\\response.py:1066\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1066\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1068\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m   1069\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torch_env\\lib\\site-packages\\urllib3\\response.py:955\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    952\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[0;32m    953\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[1;32m--> 955\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    957\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torch_env\\lib\\site-packages\\urllib3\\response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    876\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    878\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 879\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    887\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    888\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torch_env\\lib\\site-packages\\urllib3\\response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torch_env\\lib\\http\\client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torch_env\\lib\\socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torch_env\\lib\\ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torch_env\\lib\\ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from diffusers import UNet2DConditionModel\n",
    "\n",
    "#加载预训练模型的参数\n",
    "params = UNet2DConditionModel.from_pretrained(\n",
    "    'lansinuote/diffsion_from_scratch.params', subfolder='unet')\n",
    "\n",
    "unet = UNet()\n",
    "\n",
    "#in\n",
    "unet.in_vae.load_state_dict(params.conv_in.state_dict())\n",
    "unet.in_time[0].load_state_dict(params.time_embedding.linear_1.state_dict())\n",
    "unet.in_time[2].load_state_dict(params.time_embedding.linear_2.state_dict())\n",
    "\n",
    "\n",
    "#down\n",
    "def load_tf(model, param):\n",
    "    model.norm_in.load_state_dict(param.norm.state_dict())\n",
    "    model.cnn_in.load_state_dict(param.proj_in.state_dict())\n",
    "\n",
    "    model.atten1.q.load_state_dict(\n",
    "        param.transformer_blocks[0].attn1.to_q.state_dict())\n",
    "    model.atten1.k.load_state_dict(\n",
    "        param.transformer_blocks[0].attn1.to_k.state_dict())\n",
    "    model.atten1.v.load_state_dict(\n",
    "        param.transformer_blocks[0].attn1.to_v.state_dict())\n",
    "    model.atten1.out.load_state_dict(\n",
    "        param.transformer_blocks[0].attn1.to_out[0].state_dict())\n",
    "\n",
    "    model.atten2.q.load_state_dict(\n",
    "        param.transformer_blocks[0].attn2.to_q.state_dict())\n",
    "    model.atten2.k.load_state_dict(\n",
    "        param.transformer_blocks[0].attn2.to_k.state_dict())\n",
    "    model.atten2.v.load_state_dict(\n",
    "        param.transformer_blocks[0].attn2.to_v.state_dict())\n",
    "    model.atten2.out.load_state_dict(\n",
    "        param.transformer_blocks[0].attn2.to_out[0].state_dict())\n",
    "\n",
    "    model.fc0.load_state_dict(\n",
    "        param.transformer_blocks[0].ff.net[0].proj.state_dict())\n",
    "\n",
    "    model.fc1.load_state_dict(\n",
    "        param.transformer_blocks[0].ff.net[2].state_dict())\n",
    "\n",
    "    model.norm_atten0.load_state_dict(\n",
    "        param.transformer_blocks[0].norm1.state_dict())\n",
    "    model.norm_atten1.load_state_dict(\n",
    "        param.transformer_blocks[0].norm2.state_dict())\n",
    "    model.norm_act.load_state_dict(\n",
    "        param.transformer_blocks[0].norm3.state_dict())\n",
    "\n",
    "    model.cnn_out.load_state_dict(param.proj_out.state_dict())\n",
    "\n",
    "\n",
    "def load_res(model, param):\n",
    "    model.time[1].load_state_dict(param.time_emb_proj.state_dict())\n",
    "\n",
    "    model.s0[0].load_state_dict(param.norm1.state_dict())\n",
    "    model.s0[2].load_state_dict(param.conv1.state_dict())\n",
    "\n",
    "    model.s1[0].load_state_dict(param.norm2.state_dict())\n",
    "    model.s1[2].load_state_dict(param.conv2.state_dict())\n",
    "\n",
    "    if isinstance(model.res, torch.nn.Module):\n",
    "        model.res.load_state_dict(param.conv_shortcut.state_dict())\n",
    "\n",
    "\n",
    "def load_down_block(model, param):\n",
    "    load_tf(model.tf0, param.attentions[0])\n",
    "    load_tf(model.tf1, param.attentions[1])\n",
    "\n",
    "    load_res(model.res0, param.resnets[0])\n",
    "    load_res(model.res1, param.resnets[1])\n",
    "\n",
    "    model.out.load_state_dict(param.downsamplers[0].conv.state_dict())\n",
    "\n",
    "\n",
    "load_down_block(unet.down_block0, params.down_blocks[0])\n",
    "load_down_block(unet.down_block1, params.down_blocks[1])\n",
    "load_down_block(unet.down_block2, params.down_blocks[2])\n",
    "\n",
    "load_res(unet.down_res0, params.down_blocks[3].resnets[0])\n",
    "load_res(unet.down_res1, params.down_blocks[3].resnets[1])\n",
    "\n",
    "#mid\n",
    "load_tf(unet.mid_tf, params.mid_block.attentions[0])\n",
    "load_res(unet.mid_res0, params.mid_block.resnets[0])\n",
    "load_res(unet.mid_res1, params.mid_block.resnets[1])\n",
    "\n",
    "#up\n",
    "load_res(unet.up_res0, params.up_blocks[0].resnets[0])\n",
    "load_res(unet.up_res1, params.up_blocks[0].resnets[1])\n",
    "load_res(unet.up_res2, params.up_blocks[0].resnets[2])\n",
    "unet.up_in[1].load_state_dict(\n",
    "    params.up_blocks[0].upsamplers[0].conv.state_dict())\n",
    "\n",
    "\n",
    "def load_up_block(model, param):\n",
    "    load_tf(model.tf0, param.attentions[0])\n",
    "    load_tf(model.tf1, param.attentions[1])\n",
    "    load_tf(model.tf2, param.attentions[2])\n",
    "\n",
    "    load_res(model.res0, param.resnets[0])\n",
    "    load_res(model.res1, param.resnets[1])\n",
    "    load_res(model.res2, param.resnets[2])\n",
    "\n",
    "    if isinstance(model.out, torch.nn.Module):\n",
    "        model.out[1].load_state_dict(param.upsamplers[0].conv.state_dict())\n",
    "\n",
    "\n",
    "load_up_block(unet.up_block0, params.up_blocks[1])\n",
    "load_up_block(unet.up_block1, params.up_blocks[2])\n",
    "load_up_block(unet.up_block2, params.up_blocks[3])\n",
    "\n",
    "#out\n",
    "unet.out[0].load_state_dict(params.conv_norm_out.state_dict())\n",
    "unet.out[2].load_state_dict(params.conv_out.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e1df7a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# out_vae = torch.randn(1, 4, 64, 64)\n",
    "# out_encoder = torch.randn(1, 77, 768)\n",
    "# time = torch.LongTensor([26])\n",
    "\n",
    "# a = unet(out_vae=out_vae, out_encoder=out_encoder, time=time)\n",
    "# b = params(out_vae, time, out_encoder).sample\n",
    "\n",
    "# (a == b).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
