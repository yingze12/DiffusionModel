{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f5d4ac4",
   "metadata": {},
   "source": [
    "VAE 图片编码 解码模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb43719b",
   "metadata": {},
   "source": [
    "残差连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c44e94cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 10, 10])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class Resnet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dim_in, dim_out): # 定义一个残差块，输入通道数 dim_in，输出通道数 dim_out。\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        这是主分支（residual branch）s：\n",
    "        GroupNorm(32, dim_in): 对每个样本、每组通道做归一化（不依赖 batch 大小），affine=True 说明有可学习的缩放/平移参数；eps=1e-6 防止除零。\n",
    "        SiLU()：激活函数（Swish：x*sigmoid(x)），比 ReLU 平滑。\n",
    "        Conv2d(dim_in→dim_out, 3×3, padding=1): 保持空间大小不变，通道变为 dim_out。\n",
    "        GroupNorm(32, dim_out): 归一化新通道数。\n",
    "        SiLU()：再次激活。\n",
    "        Conv2d(dim_out→dim_out, 3×3, padding=1): 通道与空间尺寸都保持（通道仍 dim_out）。\n",
    "        \"\"\"\n",
    "        self.s = torch.nn.Sequential(\n",
    "            torch.nn.GroupNorm(num_groups=32,\n",
    "                               num_channels=dim_in,\n",
    "                               eps=1e-6,\n",
    "                               affine=True),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Conv2d(dim_in,\n",
    "                            dim_out,\n",
    "                            kernel_size=3,\n",
    "                            stride=1,\n",
    "                            padding=1),\n",
    "            torch.nn.GroupNorm(num_groups=32,\n",
    "                               num_channels=dim_out,\n",
    "                               eps=1e-6,\n",
    "                               affine=True),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Conv2d(dim_out,\n",
    "                            dim_out,\n",
    "                            kernel_size=3,\n",
    "                            stride=1,\n",
    "                            padding=1),\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        这是捷径分支（skip/shortcut）：只有当输入输出通道不同才用 1×1 卷积把通道从 dim_in 投影到 dim_out，以便后续逐元素相加时形状一致。\n",
    "        如果 dim_in == dim_out，捷径就是恒等映射（不需要卷积）。\n",
    "        \"\"\"\n",
    "        self.res = None\n",
    "        if dim_in != dim_out:\n",
    "            self.res = torch.nn.Conv2d(dim_in,\n",
    "                                       dim_out,\n",
    "                                       kernel_size=1,\n",
    "                                       stride=1,\n",
    "                                       padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x -> [1, 128, 10, 10]\n",
    "\n",
    "        \"\"\"\n",
    "        先保存一份输入作为捷径分支。\n",
    "        因为这里 dim_in=128 != 256=dim_out，self.res 存在，于是用 1×1 卷积把通道变到 256，空间尺寸不变：\n",
    "        res 形状变为 [1, 256, 10, 10]。\n",
    "        \"\"\"\n",
    "        res = x\n",
    "        if self.res:\n",
    "            #[1, 128, 10, 10] -> [1, 256, 10, 10]\n",
    "            res = self.res(x)\n",
    "\n",
    "        #[1, 128, 10, 10] -> [1, 256, 10, 10]\n",
    "        return res + self.s(x)\n",
    "\n",
    "\n",
    "Resnet(128, 256)(torch.randn(1, 128, 10, 10)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ac3c12",
   "metadata": {},
   "source": [
    "VAE 注意力层 初始化部分  典型的自注意力  计算过程q*k*v  单头，无mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "268cb4af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 64, 64])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Atten(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.norm = torch.nn.GroupNorm(num_channels=512,\n",
    "                                       num_groups=32,\n",
    "                                       eps=1e-6,\n",
    "                                       affine=True) # 对每个样本按通道分 32 组做归一化（每组 16 个通道），提升稳定性；affine=True 会学到每组的缩放/平移参数。\n",
    "\n",
    "        self.q = torch.nn.Linear(512, 512)\n",
    "        self.k = torch.nn.Linear(512, 512)\n",
    "        self.v = torch.nn.Linear(512, 512)\n",
    "        self.out = torch.nn.Linear(512, 512) # 把每个“空间位置”的 512 维特征映射到查询/键/值/输出空间（这里等维，单头注意力的实现方式）。\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x -> [1, 512, 64, 64]\n",
    "        res = x # 保存残差，最后做 x + res。\n",
    "\n",
    "        #norm,维度不变\n",
    "        #[1, 512, 64, 64]\n",
    "        x = self.norm(x) # 按组做标准化，缓解分布漂移、加速收敛。\n",
    "\n",
    "        #[1, 512, 64, 64] -> [1, 512, 4096] -> [1, 4096, 512]\n",
    "        x = x.flatten(start_dim=2).transpose(1, 2) # 把 64×64 个像素位置拼成长度为 4096 的“token 序列”，每个 token 的通道维是 512。\n",
    "                                                   # 此时：L=4096（序列长度），D=512（通道/嵌入维）。\n",
    "\n",
    "        #线性运算,维度不变 线性投影得到 Q/K/V（逐 token 线性变换）\n",
    "        #[1, 4096, 512]\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "\n",
    "        #[1, 4096, 512] -> [1, 512, 4096] 转置 K 以便做 batched 矩阵乘（K^T）\n",
    "        k = k.transpose(1, 2)\n",
    "\n",
    "        #[1, 4096, 512] * [1, 512, 4096] -> [1, 4096, 4096]\n",
    "        #0.044194173824159216 = 1 / 512**0.5\n",
    "        #atten = q.bmm(k) * 0.044194173824159216\n",
    "\n",
    "        # 期望：attn_logits = q @ k * (1 / sqrt(D))\n",
    "        # 这里用 baddbmm 实现：C = beta*C + alpha*(q @ k)\n",
    "\n",
    "        #照理来说应该是等价的,但是却有很小的误差\n",
    "        atten = torch.baddbmm(torch.empty(1, 4096, 4096, device=q.device), # 初始 C（因为 beta=0，所以内容会被忽略）\n",
    "                              q, # [1, 4096, 512]\n",
    "                              k, # [1, 512, 4096]\n",
    "                              beta=0,\n",
    "                              alpha=0.044194173824159216) # ≈ 1/sqrt(512)\n",
    "\n",
    "        atten = torch.softmax(atten, dim=2) # 对“keys 维”（最后一维）做 softmax：每个 query 的 4096 个权重求和为 1。\n",
    "\n",
    "        #[1, 4096, 4096] * [1, 4096, 512] -> [1, 4096, 512]\n",
    "        atten = atten.bmm(v)\n",
    "\n",
    "        #线性运算,维度不变\n",
    "        #[1, 4096, 512]\n",
    "        atten = self.out(atten)\n",
    "\n",
    "        #[1, 4096, 512] -> [1, 512, 4096] -> [1, 512, 64, 64]\n",
    "        atten = atten.transpose(1, 2).reshape(-1, 512, 64, 64)\n",
    "\n",
    "        #残差连接,维度不变\n",
    "        #[1, 512, 64, 64]\n",
    "        atten = atten + res\n",
    "\n",
    "        return atten\n",
    "\n",
    "\n",
    "Atten()(torch.randn(1, 512, 64, 64)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee4ba23",
   "metadata": {},
   "source": [
    "工具层 增加一行一列0\n",
    "\n",
    "F.pad(x, (0, 1, 0, 1), mode='constant', value=0) 对 4D 张量 [N, C, H, W] 的 宽、高 维做填充：\n",
    "\n",
    "参数顺序（2D）是 (left, right, top, bottom)\n",
    "\n",
    "这里是 (0, 1, 0, 1)：左 0、右 +1、上 0、下 +1\n",
    "\n",
    "结果：[1, 2, 5, 5] → [1, 2, 6, 6]\n",
    "\n",
    "因为输入是全 1，填充后最后一列和最后一行是 0，其余仍为 1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdc82ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 1., 1., 1., 0.],\n",
       "          [1., 1., 1., 1., 1., 0.],\n",
       "          [1., 1., 1., 1., 1., 0.],\n",
       "          [1., 1., 1., 1., 1., 0.],\n",
       "          [1., 1., 1., 1., 1., 0.],\n",
       "          [0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 0.],\n",
       "          [1., 1., 1., 1., 1., 0.],\n",
       "          [1., 1., 1., 1., 1., 0.],\n",
       "          [1., 1., 1., 1., 1., 0.],\n",
       "          [1., 1., 1., 1., 1., 0.],\n",
       "          [0., 0., 0., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Pad(torch.nn.Module):\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.pad(x, (0, 1, 0, 1),\n",
    "                                       mode='constant',\n",
    "                                       value=0)\n",
    "\n",
    "\n",
    "Pad()(torch.ones(1, 2, 5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3183f8",
   "metadata": {},
   "source": [
    "VAE模型  计算过程  编码，投影，解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53ef92d8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 512, 512])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VAE(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            #in\n",
    "            torch.nn.Conv2d(3, 128, kernel_size=3, stride=1, padding=1),\n",
    "\n",
    "            #down\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(128, 128),\n",
    "                Resnet(128, 128),\n",
    "                torch.nn.Sequential(\n",
    "                    Pad(),\n",
    "                    torch.nn.Conv2d(128, 128, 3, stride=2, padding=0),\n",
    "                ),\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(128, 256),\n",
    "                Resnet(256, 256),\n",
    "                torch.nn.Sequential(\n",
    "                    Pad(),\n",
    "                    torch.nn.Conv2d(256, 256, 3, stride=2, padding=0),\n",
    "                ),\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(256, 512),\n",
    "                Resnet(512, 512),\n",
    "                torch.nn.Sequential(\n",
    "                    Pad(),\n",
    "                    torch.nn.Conv2d(512, 512, 3, stride=2, padding=0),\n",
    "                ),\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(512, 512),\n",
    "                Resnet(512, 512),\n",
    "            ),\n",
    "\n",
    "            #mid\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(512, 512),\n",
    "                Atten(),\n",
    "                Resnet(512, 512),\n",
    "            ),\n",
    "\n",
    "            #out\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.GroupNorm(num_channels=512, num_groups=32, eps=1e-6),\n",
    "                torch.nn.SiLU(),\n",
    "                torch.nn.Conv2d(512, 8, 3, padding=1),\n",
    "            ),\n",
    "\n",
    "            #正态分布层\n",
    "            torch.nn.Conv2d(8, 8, 1),\n",
    "        )\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            #正态分布层\n",
    "            torch.nn.Conv2d(4, 4, 1),\n",
    "\n",
    "            #in\n",
    "            torch.nn.Conv2d(4, 512, kernel_size=3, stride=1, padding=1),\n",
    "\n",
    "            #middle\n",
    "            torch.nn.Sequential(Resnet(512, 512), Atten(), Resnet(512, 512)),\n",
    "\n",
    "            #up\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(512, 512),\n",
    "                Resnet(512, 512),\n",
    "                Resnet(512, 512),\n",
    "                torch.nn.Upsample(scale_factor=2.0, mode='nearest'),\n",
    "                torch.nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(512, 512),\n",
    "                Resnet(512, 512),\n",
    "                Resnet(512, 512),\n",
    "                torch.nn.Upsample(scale_factor=2.0, mode='nearest'),\n",
    "                torch.nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(512, 256),\n",
    "                Resnet(256, 256),\n",
    "                Resnet(256, 256),\n",
    "                torch.nn.Upsample(scale_factor=2.0, mode='nearest'),\n",
    "                torch.nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(256, 128),\n",
    "                Resnet(128, 128),\n",
    "                Resnet(128, 128),\n",
    "            ),\n",
    "\n",
    "            #out\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.GroupNorm(num_channels=128, num_groups=32, eps=1e-6),\n",
    "                torch.nn.SiLU(),\n",
    "                torch.nn.Conv2d(128, 3, 3, padding=1),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def sample(self, h):\n",
    "        # h -> [1, 8, 64, 64]  # h: [B=1, 8, 64, 64]，每个空间位置都有 8 维值：前 4 维是 mean，后 4 维是 logvar\n",
    "\n",
    "        #[1, 4, 64, 64]\n",
    "        mean = h[:, :4] # 取前 4 个通道当作 μ，形状 [1, 4, 64, 64]\n",
    "        logvar = h[:, 4:]  # 取后 4 个通道当作 log σ²，形状 [1, 4, 64, 64]\n",
    "        std = logvar.exp()**0.5 # 先做 exp 得到 σ²，再开方得到 σ（数值等价于 torch.exp(0.5*logvar)）\n",
    "                                # 形状 [1, 4, 64, 64]\n",
    "\n",
    "        #[1, 4, 64, 64]\n",
    "        h = torch.randn(mean.shape, device=mean.device) # 采样 ε ~ N(0, I)，形状 [1, 4, 64, 64]\n",
    "        h = mean + std * h # 重参数化：z = μ + σ ⊙ ε，梯度可穿过 μ 和 σ\n",
    "\n",
    "        return h # 返回 z，形状 [1, 4, 64, 64]\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x -> [1, 3, 512, 512]\n",
    "\n",
    "        #[1, 3, 512, 512] -> [1, 8, 64, 64]\n",
    "        h = self.encoder(x) # 编码：输出 [1, 8, 64, 64]\n",
    "                            # 按你的网络设计：这 8 个通道 = 4(μ) + 4(log σ²)\n",
    "\n",
    "        #[1, 8, 64, 64] -> [1, 4, 64, 64]\n",
    "        h = self.sample(h) # 采样潜变量：把 [1, 8, 64, 64] -> [1, 4, 64, 64]（z）\n",
    "\n",
    "        #[1, 4, 64, 64] -> [1, 3, 512, 512]\n",
    "        h = self.decoder(h) # 解码：将 z 还原到图像空间，得到 [1, 3, 512, 512]\n",
    "\n",
    "        return h # 返回重建图像，形状与输入一致\n",
    "\n",
    "\n",
    "VAE()(torch.randn(1, 3, 512, 512)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ff6fdb",
   "metadata": {},
   "source": [
    "准备加载  预训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8559249f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dc5bf7a03604dbc9a3b572685c7bd27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/335M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\torch_env\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yingz\\.cache\\huggingface\\diffusers. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96afaa240c44d5b9783d89588159c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/728 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'scaling_factor': 0.18215} were passed to AutoencoderKL, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusers import AutoencoderKL\n",
    "\n",
    "#加载预训练模型的参数\n",
    "params = AutoencoderKL.from_pretrained(\n",
    "    'lansinuote/diffsion_from_scratch.params', subfolder='vae')\n",
    "\n",
    "vae = VAE()\n",
    "\n",
    "\n",
    "def load_res(model, param):\n",
    "    model.s[0].load_state_dict(param.norm1.state_dict())\n",
    "    model.s[2].load_state_dict(param.conv1.state_dict())\n",
    "    model.s[3].load_state_dict(param.norm2.state_dict())\n",
    "    model.s[5].load_state_dict(param.conv2.state_dict())\n",
    "\n",
    "    if isinstance(model.res, torch.nn.Module):\n",
    "        model.res.load_state_dict(param.conv_shortcut.state_dict())\n",
    "\n",
    "\n",
    "def conv1x1_to_linear_(linear: torch.nn.Linear, conv: torch.nn.Conv2d):\n",
    "    \"\"\"\n",
    "    将 1x1 Conv2d 权重拷贝到 Linear：\n",
    "    [out, in, 1, 1] -> [out, in]\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        w = conv.weight.data\n",
    "        if w.ndim == 4:\n",
    "            w = w.squeeze(-1).squeeze(-1)      # [O, I, 1, 1] -> [O, I]\n",
    "        linear.weight.copy_(w)\n",
    "        # 只有两边都有 bias 才拷贝\n",
    "        if (linear.bias is not None) and (conv.bias is not None):\n",
    "            linear.bias.copy_(conv.bias.data)\n",
    "\n",
    "def load_atten(model, param):\n",
    "    \"\"\"\n",
    "    model: 你的 Atten 实例（q/k/v/out 是 Linear）\n",
    "    param: diffusers 的 Attention 模块（query/key/value/proj_attn 是 Conv2d 1x1）\n",
    "    \"\"\"\n",
    "    model.norm.load_state_dict(param.group_norm.state_dict())\n",
    "    conv1x1_to_linear_(model.q,   param.query)\n",
    "    conv1x1_to_linear_(model.k,   param.key)\n",
    "    conv1x1_to_linear_(model.v,   param.value)\n",
    "    conv1x1_to_linear_(model.out, param.proj_attn)\n",
    "\n",
    "\n",
    "#encoder.in\n",
    "vae.encoder[0].load_state_dict(params.encoder.conv_in.state_dict())\n",
    "\n",
    "#encoder.down\n",
    "for i in range(4):\n",
    "    load_res(vae.encoder[i + 1][0], params.encoder.down_blocks[i].resnets[0])\n",
    "    load_res(vae.encoder[i + 1][1], params.encoder.down_blocks[i].resnets[1])\n",
    "\n",
    "    if i != 3:\n",
    "        vae.encoder[i + 1][2][1].load_state_dict(\n",
    "            params.encoder.down_blocks[i].downsamplers[0].conv.state_dict())\n",
    "\n",
    "#encoder.mid\n",
    "load_res(vae.encoder[5][0], params.encoder.mid_block.resnets[0])\n",
    "load_res(vae.encoder[5][2], params.encoder.mid_block.resnets[1])\n",
    "load_atten(vae.encoder[5][1], params.encoder.mid_block.attentions[0])\n",
    "\n",
    "#encoder.out\n",
    "vae.encoder[6][0].load_state_dict(params.encoder.conv_norm_out.state_dict())\n",
    "vae.encoder[6][2].load_state_dict(params.encoder.conv_out.state_dict())\n",
    "\n",
    "#encoder.正态分布层\n",
    "vae.encoder[7].load_state_dict(params.quant_conv.state_dict())\n",
    "\n",
    "#decoder.正态分布层\n",
    "vae.decoder[0].load_state_dict(params.post_quant_conv.state_dict())\n",
    "\n",
    "#decoder.in\n",
    "vae.decoder[1].load_state_dict(params.decoder.conv_in.state_dict())\n",
    "\n",
    "#decoder.mid\n",
    "load_res(vae.decoder[2][0], params.decoder.mid_block.resnets[0])\n",
    "load_res(vae.decoder[2][2], params.decoder.mid_block.resnets[1])\n",
    "load_atten(vae.decoder[2][1], params.decoder.mid_block.attentions[0])\n",
    "\n",
    "#decoder.up\n",
    "for i in range(4):\n",
    "    load_res(vae.decoder[i + 3][0], params.decoder.up_blocks[i].resnets[0])\n",
    "    load_res(vae.decoder[i + 3][1], params.decoder.up_blocks[i].resnets[1])\n",
    "    load_res(vae.decoder[i + 3][2], params.decoder.up_blocks[i].resnets[2])\n",
    "\n",
    "    if i != 3:\n",
    "        vae.decoder[i + 3][4].load_state_dict(\n",
    "            params.decoder.up_blocks[i].upsamplers[0].conv.state_dict())\n",
    "\n",
    "#decoder.out\n",
    "vae.decoder[7][0].load_state_dict(params.decoder.conv_norm_out.state_dict())\n",
    "vae.decoder[7][2].load_state_dict(params.decoder.conv_out.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac79f0f",
   "metadata": {},
   "source": [
    "检查模型  构建准确无误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86ee9ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.randn(1, 3, 512, 512)\n",
    "\n",
    "a = vae.encoder(data)\n",
    "b = params.encode(data).latent_dist.parameters\n",
    "\n",
    "(a == b).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87771216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.randn(1, 4, 64, 64)\n",
    "\n",
    "a = vae.decoder(data)\n",
    "b = params.decode(data).sample\n",
    "\n",
    "(a == b).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
