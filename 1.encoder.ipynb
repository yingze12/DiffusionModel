{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53e3d8bb",
   "metadata": {},
   "source": [
    "代码作用：把输入的 token 序列做词向量嵌入（token embedding）并加上位置嵌入（positional embedding）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60cba92f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 77, 768])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Embed(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = torch.nn.Embedding(49408, 768)\n",
    "        # 定义词嵌入层（可训练参数）。\n",
    "        # 形状说明：Embedding(num_embeddings, embedding_dim)\n",
    "        # num_embeddings=49408：词表大小（能索引的 token 个数）。\n",
    "        # embedding_dim=768：每个 token 的向量维度。\n",
    "\n",
    "        self.pos_embed = torch.nn.Embedding(77, 768)\n",
    "        # 定义位置嵌入层（同样可训练）。\n",
    "        # 这里序列最大长度设为 77，所以位置索引范围是 [0, 76]，每个位置一条 768 维向量。\n",
    "\n",
    "        self.register_buffer('pos_ids', torch.arange(77).unsqueeze(dim=0))\n",
    "        # 注册一个名为 pos_ids 的buffer（不是参数，没有梯度，但会随模型保存/加载、随 .to(device) 自动搬运到同一设备）。\n",
    "        # torch.arange(77) 生成 [0, 1, 2, ..., 76]，形状 [77]。\n",
    "        # .unsqueeze(dim=0) 变成形状 [1, 77]，方便后续与 batch 做广播相加。\n",
    "\n",
    "    def forward(self, input_ids): # 前向函数，input_ids 是输入的 token 索引张量。\n",
    "        #input_ids -> [b, 77] 约定输入的序列长度是 77（与位置嵌入一致）。batch 大小记为 b。\n",
    "\n",
    "        embed = self.embed(input_ids) # 查词嵌入。 形状变化：[b, 77] → [b, 77, 768]。\n",
    "\n",
    "        pos_embed = self.pos_embed(self.pos_ids) # 用事先存好的 pos_ids（形状 [1, 77]）查位置嵌入。 形状变化：[1, 77] → [1, 77, 768]。 注意这里之所以把 pos_ids 做成 [1, 77]，就是为了后面能在 batch 维上广播。\n",
    "\n",
    "        return embed + pos_embed # 把词嵌入与位置嵌入相加得到最终嵌入。 由于 embed 是 [b, 77, 768]，pos_emb 是 [1, 77, 768]，广播后相加结果是 [b, 77, 768]。\n",
    "\n",
    "\n",
    "Embed()(torch.ones(2, 77).long()).shape # 建一个 Embed 实例，并用一个形状为 [2, 77] 的整型输入测试（这里用的是全 1 的 fake 索引）。 输出形状应为 [2, 77, 768]，与上面的推导一致。\n",
    "\n",
    "# “有 2 句话 × 每句话 77 个位置 × 每个位置一个 768 维向量”。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190c49d2",
   "metadata": {},
   "source": [
    "encoder注意力层 典型的多头注意力 计算过程mask(q*k)*v 带向后的注意力mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1820e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 77, 768])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Atten(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    定义了一个最小可用的多头自注意力（Multi-Head Self-Attention）模块的核心。\n",
    "    共有四个线性层：\n",
    "    q/k/v：把输入特征投影到查询/键/值空间（维度都还是 768）。\n",
    "    out：把多头拼接后的结果再映射回 768 维。\n",
    "    这里没有显式的 nn.MultiheadAttention，而是手动实现多头拆分与计算。\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.q = torch.nn.Linear(768, 768)\n",
    "        self.k = torch.nn.Linear(768, 768)\n",
    "        self.v = torch.nn.Linear(768, 768)\n",
    "        self.out = torch.nn.Linear(768, 768)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x is input sequence length -> [b, 77, 768]\n",
    "\n",
    "        b = x.shape[0] # batch size\n",
    "\n",
    "        \"\"\"\n",
    "        通过三组线性层得到 q/k/v，形状仍是 [b, 77, 768]。\n",
    "        关键点：q 乘了 0.125。\n",
    "        这是缩放因子，等于 1 / sqrt(d_head)。本实现中单头维度稍后会是 64，所以 1/sqrt(64)=1/8=0.125。\n",
    "        作用：与标准 Scaled Dot-Product Attention 一致，避免点积值过大导致 softmax 退化/梯度不稳定。\n",
    "        \"\"\"\n",
    "        #维度不变,得到q,k,v三个矩阵\n",
    "        #[b, 77, 768]\n",
    "        q = self.q(x) * 0.125\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "\n",
    "        \"\"\"\n",
    "        把 768 维拆成 12 个头、每头 64 维（因为 12*64=768）。\n",
    "        步骤说明：\n",
    "        reshape(b, 77, 12, 64)：显式引入头数维度。\n",
    "        transpose(1, 2)：把维度变为 [b, 12, 77, 64]，即先按头再按序列。\n",
    "        reshape(b*12, 77, 64)：把 batch 和 head 合并，方便用 torch.bmm 做批量矩阵乘法。\n",
    "        这样每个头就像一个“独立样本”。\n",
    "        \"\"\"\n",
    "        #拆分注意力头\n",
    "        #[b, 77, 768] -> [b, 77, 12, 64] -> [b, 12, 77, 64] -> [b*12, 77, 64]\n",
    "        q = q.reshape(b, 77, 12, 64).transpose(1, 2).reshape(b * 12, 77, 64)\n",
    "        k = k.reshape(b, 77, 12, 64).transpose(1, 2).reshape(b * 12, 77, 64)\n",
    "        v = v.reshape(b, 77, 12, 64).transpose(1, 2).reshape(b * 12, 77, 64)\n",
    "\n",
    "        #计算qk乘积 对每个头做 Q @ K^T，得到注意力分数矩阵（未经 softmax）。 每个头得到一个 [77, 77] 的相关性矩阵：第 i 个 token 对第 j 个 token 的注意力打分。\n",
    "        #[b*12, 77, 64] * [b*12, 64, 77] -> [b*12, 77, 77] \n",
    "        attn = torch.bmm(q, k.transpose(1, 2))\n",
    "\n",
    "        #[b*12, 77, 77] -> [b, 12, 77, 77] 把合并的 b*12 维拆回 [b, 12, ...]，以便加 mask。\n",
    "        attn = attn.reshape(b, 12, 77, 77)\n",
    "\n",
    "        \"\"\"\n",
    "        生成一个形状为 [b, 1, 77, 77] 的 mask，用于因果（下三角）注意力：\n",
    "        fill_(-inf) 先全填为 -∞。\n",
    "        triu_(1)（保留主对角线上方的上三角，其他置 0）：\n",
    "        结果是严格上三角部分为 -∞，主对角线及其下方为 0。\n",
    "        含义：位置 t 只能看见自己以及之前的位置（对角线及以下），不允许看“未来”（上三角被设为 -∞）。\n",
    "        \"\"\"\n",
    "        #覆盖mask\n",
    "        def get_mask(b):\n",
    "            mask = torch.empty(b, 77, 77)\n",
    "\n",
    "            #上三角的部分置为负无穷\n",
    "            mask.fill_(-float('inf'))\n",
    "\n",
    "            #对角线和以下的位置为0\n",
    "            mask.triu_(1)\n",
    "\n",
    "            return mask.unsqueeze(1) # [b, 77, 77] - > [b, 1, 77, 77]\n",
    "\n",
    "        \"\"\"\n",
    "        把 mask 加到注意力分数上：\n",
    "        上三角加上 -∞，在后续 softmax 里会变成 0 概率（被完全屏蔽）。\n",
    "        对角线及以下位置加 0，不受影响。\n",
    "        .to(attn.device) 确保与 attn 在同一设备（CPU/GPU）上。\n",
    "        \"\"\"\n",
    "        #[b, 12, 77, 77] + [b, 1, 77, 77] -> [b, 12, 77, 77]\n",
    "        attn = attn + get_mask(attn.shape[0]).to(attn.device)\n",
    "\n",
    "        #[b, 12, 77, 77] -> [b*12, 77, 77] 再次把 batch 和 head 合并，方便用 bmm。\n",
    "        attn = attn.reshape(b * 12, 77, 77)\n",
    "\n",
    "        #计算softmax,被mask的部分值为0\n",
    "        attn = attn.softmax(dim=-1) # 在最后一个维度（键的维度）做 softmax，得到每个查询位置对所有键位置的注意力权重。\n",
    "        # 由于上三角是 -∞，softmax 后这些位置权重为 0，实现因果屏蔽。\n",
    "\n",
    "        #计算和v的乘积\n",
    "        #[b*12, 77, 77] * [b*12, 77, 64] -> [b*12, 77, 64] 用注意力权重对 V 做加权求和，得到每个头的上下文向量。\n",
    "        attn = torch.bmm(attn, v)\n",
    "\n",
    "        \"\"\"\n",
    "        把头维合回来：\n",
    "        先还原为 [b, 12, 77, 64]\n",
    "        转置为 [b, 77, 12, 64]（更便于拼接）\n",
    "        最后拼成 [b, 77, 768]（12 头 × 64 维 = 768）\n",
    "        \"\"\"\n",
    "        #[b*12, 77, 64] -> [b, 12, 77, 64] -> [b, 77, 12, 64] -> [b, 77, 768]\n",
    "        attn = attn.reshape(b, 12, 77, 64).transpose(1, 2).reshape(b, 77, 768)\n",
    "\n",
    "        #线性输出,维度不变\n",
    "        #[b, 77, 768]\n",
    "        return self.out(attn)\n",
    "\n",
    "\n",
    "Atten()(torch.randn(2, 77, 768)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97777859",
   "metadata": {},
   "source": [
    "一层编码器\n",
    "激活函数quick gelu:\n",
    "x * sigmoid(1.702 * x)\n",
    "\n",
    "Transformer 的一个“预归一化（Pre-LN）Block”：自注意力残差块 + 前馈（MLP）残差块，激活使用 QuickGELU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07982501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 77, 768])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ClipEncoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.s1 = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(768),\n",
    "            Atten(),\n",
    "        )\n",
    "\n",
    "        self.s2 = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(768),\n",
    "            torch.nn.Linear(768, 3072),\n",
    "        )\n",
    "\n",
    "        self.s3 = torch.nn.Linear(3072, 768)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x -> [2, 77, 768]\n",
    "\n",
    "        #维度不变\n",
    "        #[2, 77, 768]\n",
    "        x = x + self.s1(x)\n",
    "\n",
    "        #[2, 77, 768]\n",
    "        res = x\n",
    "\n",
    "        #[2, 77, 768] -> [2, 77, 3072]\n",
    "        x = self.s2(x)\n",
    "\n",
    "        #维度不变\n",
    "        #[2, 77, 3072]\n",
    "        x = x * (x * 1.702).sigmoid()\n",
    "\n",
    "        #[2, 77, 3072] -> [2, 77, 768]\n",
    "        return res + self.s3(x)\n",
    "\n",
    "\n",
    "ClipEncoder()(torch.randn(2, 77, 768)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc306ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 77, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#经过优化之后的代码量少得吓人...\n",
    "encoder = torch.nn.Sequential(\n",
    "    Embed(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    torch.nn.LayerNorm(768),\n",
    ")\n",
    "\n",
    "encoder(torch.ones(2, 77).long()).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea78e3a",
   "metadata": {},
   "source": [
    "加载预训练参数\n",
    "\n",
    "现在我们有了encoder模型，但是它当中所有的参数还是随机初始化的，为了帮助我们的训练过程能够更快的进行收敛，我们需要从一个已经训练的模型当中，把这些参数给加载过来，这里我们从这个checkpoint来加载一个预训练的模型，然后从这个预训练的模型当中把它所有的参数给它抽取出来，抽取到我们自己的模型当中，我们用预训练模型当中的参数来初始化我们自己的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73142c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\torch_env\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4bc50e98b82494dafbf8614a792f174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\torch_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yingz\\.cache\\huggingface\\hub\\models--lansinuote--diffsion_from_scratch.params. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be41fb752fa40e2b63762bc4b448d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/492M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import CLIPTextModel # 从 transformers（Hugging Face）库中导入 CLIPTextModel，它是 CLIP 文本编码器（Transformer 堆叠）的实现与权重加载入口。\n",
    "\n",
    "#加载预训练模型的参数 \n",
    "\"\"\"\n",
    "从 Hugging Face Hub 仓库 'lansinuote/diffsion_from_scratch.params' 的子文件夹 text_encoder 下载并实例化一个 CLIPTextModel。\n",
    "变量名叫 params，但它其实是完整模型实例（包含子模块与参数），不是“纯粹的参数字典”。\n",
    "典型 CLIP 文本分支结构：Token Embedding → Position Embedding → 12 层 Transformer Encoder Block → Final LayerNorm。你的后续代码就是把这些预训练权重逐模块拷贝到你自定义的 encoder 结构中。\n",
    "\"\"\"\n",
    "params = CLIPTextModel.from_pretrained(\n",
    "    'lansinuote/diffsion_from_scratch.params', subfolder='text_encoder')\n",
    "\n",
    "\"\"\"\n",
    "将 Hugging Face 模型中 token embedding（形如 [vocab_size, hidden_size]，常见 [vocab, 768]）的权重，拷贝到你自定义的 encoder[0].embed。\n",
    "load_state_dict 会严格匹配参数名与形状；这里你是对子模块调用，等价于把该子模块的权重一一覆盖。\n",
    "\"\"\"\n",
    "#词编码\n",
    "encoder[0].embed.load_state_dict(\n",
    "    params.text_model.embeddings.token_embedding.state_dict())\n",
    "#位置编码\n",
    "encoder[0].pos_embed.load_state_dict(\n",
    "    params.text_model.embeddings.position_embedding.state_dict())\n",
    "\n",
    "#12层编码层\n",
    "\"\"\"\n",
    "循环 12 次（i = 0..11），对应 CLIP 文本编码器的 12 层 Transformer Block。\n",
    "你后续访问的是 encoder[i + 1]，因此层索引映射为：\n",
    "预训练第 0 层 → 你的 encoder[1]\n",
    "预训练第 11 层 → 你的 encoder[12]\n",
    "\"\"\"\n",
    "for i in range(12):\n",
    "\n",
    "    #第一层norm \n",
    "    encoder[i + 1].s1[0].load_state_dict(\n",
    "        params.text_model.encoder.layers[i].layer_norm1.state_dict())\n",
    "\n",
    "    #注意力q矩阵\n",
    "    encoder[i + 1].s1[1].q.load_state_dict(\n",
    "        params.text_model.encoder.layers[i].self_attn.q_proj.state_dict())\n",
    "\n",
    "    #注意力k矩阵\n",
    "    encoder[i + 1].s1[1].k.load_state_dict(\n",
    "        params.text_model.encoder.layers[i].self_attn.k_proj.state_dict())\n",
    "\n",
    "    #注意力v矩阵\n",
    "    encoder[i + 1].s1[1].v.load_state_dict(\n",
    "        params.text_model.encoder.layers[i].self_attn.v_proj.state_dict())\n",
    "\n",
    "    #注意力out\n",
    "    encoder[i + 1].s1[1].out.load_state_dict(\n",
    "        params.text_model.encoder.layers[i].self_attn.out_proj.state_dict())\n",
    "\n",
    "    #第二层norm\n",
    "    encoder[i + 1].s2[0].load_state_dict(\n",
    "        params.text_model.encoder.layers[i].layer_norm2.state_dict())\n",
    "\n",
    "    #mlp第一层fc\n",
    "    encoder[i + 1].s2[1].load_state_dict(\n",
    "        params.text_model.encoder.layers[i].mlp.fc1.state_dict())\n",
    "\n",
    "    #mlp第二层fc\n",
    "    encoder[i + 1].s3.load_state_dict(\n",
    "        params.text_model.encoder.layers[i].mlp.fc2.state_dict())\n",
    "\n",
    "#输出norm\n",
    "encoder[13].load_state_dict(params.text_model.final_layer_norm.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2bab9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = encoder(torch.arange(77).unsqueeze(dim=0))\n",
    "# b = params(torch.arange(77).unsqueeze(dim=0)).last_hidden_state\n",
    "\n",
    "# (a == b).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
