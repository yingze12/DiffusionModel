{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22ff33e2",
   "metadata": {},
   "source": [
    "加载两个工具类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3af62b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DiffusionPipeline\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torch_env\\lib\\site-packages\\diffusers\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.12.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfigMixin\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      5\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m      6\u001b[0m     is_flax_available,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     logging,\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torch_env\\lib\\site-packages\\diffusers\\configuration_utils.py:34\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPError\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DIFFUSERS_CACHE, HUGGINGFACE_CO_RESOLVE_ENDPOINT, DummyObject, deprecate, logging\n\u001b[0;32m     37\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     39\u001b[0m _re_configuration_file \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.(.*)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torch_env\\lib\\site-packages\\diffusers\\utils\\__init__.py:68\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseOutput\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpil_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PIL_INTERPOLATION\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m randn_tensor\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     73\u001b[0m         floats_tensor,\n\u001b[0;32m     74\u001b[0m         load_hf_numpy,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m         torch_device,\n\u001b[0;32m     84\u001b[0m     )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torch_env\\lib\\site-packages\\diffusers\\utils\\torch_utils.py:24\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_torch_available\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     26\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrandn_tensor\u001b[39m(\n\u001b[0;32m     30\u001b[0m     shape: Union[Tuple, List],\n\u001b[0;32m     31\u001b[0m     generator: Optional[Union[List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Generator\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Generator\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     layout: Optional[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.layout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     35\u001b[0m ):\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\__init__.py:1919\u001b[0m\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m library\n\u001b[0;32m   1918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m-> 1919\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# Enable CUDA Sanitizer\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTORCH_CUDA_SANITIZER\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\_meta_registrations.py:821\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcloneBatchedColumnMajor\u001b[39m(src: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m src\u001b[38;5;241m.\u001b[39mmT\u001b[38;5;241m.\u001b[39mclone(memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcontiguous_format)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 821\u001b[0m \u001b[38;5;129m@register_meta\u001b[39m(\u001b[43maten\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cholesky_solve_helper\u001b[49m)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;129m@out_wrapper\u001b[39m()\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cholesky_solve_helper\u001b[39m(\u001b[38;5;28mself\u001b[39m: Tensor, A: Tensor, upper: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cloneBatchedColumnMajor(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    827\u001b[0m \u001b[38;5;129m@register_meta\u001b[39m(aten\u001b[38;5;241m.\u001b[39mcholesky_solve)\n\u001b[0;32m    828\u001b[0m \u001b[38;5;129m@out_wrapper\u001b[39m()\n\u001b[0;32m    829\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcholesky_solve\u001b[39m(\u001b[38;5;28mself\u001b[39m: Tensor, A: Tensor, upper: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\_ops.py:920\u001b[0m, in \u001b[0;36m_OpNamespace.__getattr__\u001b[1;34m(self, op_name)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m     op, overload_names \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_jit_get_operation(qualified_op_name)\n\u001b[1;32m--> 920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m:\n\u001b[0;32m    921\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    922\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_OpNamespace\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    923\u001b[0m         )\n\u001b[0;32m    924\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    925\u001b[0m     \u001b[38;5;66;03m# Turn this into AttributeError so getattr(obj, key, default)\u001b[39;00m\n\u001b[0;32m    926\u001b[0m     \u001b[38;5;66;03m# works (this is called by TorchScript with __origin__)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 从 🤗 Diffusers 导入 DiffusionPipeline（把 UNet、VAE、文本编码器、调度器等组件打包到一起的一站式推理管道）。\n",
    "# 导入 PyTorch。\n",
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 通过 from_pretrained 从给定仓库/路径加载一个已经配好的扩散模型管道（权重和配置）。\n",
    "# safety_checker=None 关闭 NSFW 安全过滤器（生成内容将不做额外审查）。\n",
    "# 这一步会把文本编码器、UNet、VAE、scheduler、tokenizer等子模块都装到 pipeline 里（默认在 CPU 上）。\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    'lansinuote/diffsion_from_scratch.params', safety_checker=None)\n",
    "\n",
    "scheduler = pipeline.scheduler # 是往图片中参入噪声的工具类\n",
    "tokenizer = pipeline.tokenizer\n",
    "\n",
    "del pipeline # 删除大而全的 pipeline 对象本身以释放内存（权重大、占 RAM/显存）。\n",
    "             # 已经单独拿出来的 scheduler 与 tokenizer 只是独立引用，不会被删。\n",
    "\n",
    "# 在 Jupyter/REPL 中，单独一行写多个变量会回显一个元组 (device, scheduler, tokenizer)，便于你看当前三者的值/类型。\n",
    "# 在普通 .py 文件里，这一行不会打印任何东西（除非 print(...)）。\n",
    "device, scheduler, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04403f4",
   "metadata": {},
   "source": [
    "对数据当中的图片和文本分别进行编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd207966",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration lansinuote--diffsion_from_scratch-34318fc75271f5a0\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/lansinuote___parquet/lansinuote--diffsion_from_scratch-34318fc75271f5a0/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/lansinuote___parquet/lansinuote--diffsion_from_scratch-34318fc75271f5a0/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-314ecfdae0cf40d9.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['pixel_values', 'input_ids'],\n",
       "     num_rows: 833\n",
       " }),\n",
       " {'pixel_values': tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           ...,\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "  \n",
       "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           ...,\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "  \n",
       "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           ...,\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.]]]),\n",
       "  'input_ids': tensor([49406,   320,  3610,   539,   320,  1901,  9528,   593,   736,  3095,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407])})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torchvision\n",
    "\n",
    "#加载数据集\n",
    "dataset = load_dataset(path='lansinuote/diffsion_from_scratch', split='train')\n",
    "\n",
    "#图像增强模块\n",
    "compose = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(\n",
    "        512, interpolation=torchvision.transforms.InterpolationMode.BILINEAR),\n",
    "    torchvision.transforms.CenterCrop(512),\n",
    "    #torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.5], [0.5]),\n",
    "])\n",
    "\n",
    "\n",
    "def f(data):\n",
    "    #应用图像增强\n",
    "    pixel_values = [compose(i) for i in data['image']]\n",
    "    # map(..., batched=True) 时，data['image'] 是一个批次的列表（长度≤batch_size），对每张图做 compose，得到一堆 [C,512,512] 的张量。\n",
    "\n",
    "    #文字编码\n",
    "    input_ids = tokenizer.batch_encode_plus(data['text'],\n",
    "                                            padding='max_length',\n",
    "                                            truncation=True,\n",
    "                                            max_length=77).input_ids # 批量把文本转成 token id：\n",
    "                                                # 固定 长度 77（CLIP 常用长度），不足则用 pad 补到 77，过长则截断；\n",
    "                                                # input_ids 是 List[List[int]]，每条样本 77 个 id（末尾很多是 pad id）。\n",
    "\n",
    "    return {'pixel_values': pixel_values, 'input_ids': input_ids}\n",
    "    # 返回给 datasets，新列叫 pixel_values 和 input_ids。datasets 会据此推断特征类型与形状。\n",
    "\n",
    "\"\"\"\n",
    "batched=True：f 一次处理一个批次，更高效；\n",
    "batch_size=100：每批 100 条数据传给 f；\n",
    "num_proc=1：单进程映射（设成>1 可多进程并行）；\n",
    "remove_columns=['image','text']：丢掉原始的 image/text 两列，避免重复存储，最后只保留处理后的两列。\n",
    "\"\"\"\n",
    "dataset = dataset.map(f,\n",
    "                      batched=True,\n",
    "                      batch_size=100,\n",
    "                      num_proc=1,\n",
    "                      remove_columns=['image', 'text'])\n",
    "\n",
    "\"\"\"\n",
    "指定取样本/批次时，把这些列自动转成 PyTorch 张量：\n",
    "pixel_values：torch.float32，形状 [C,512,512]；\n",
    "input_ids：torch.int64，形状 [77]。\n",
    "\"\"\"\n",
    "dataset.set_format(type='torch')\n",
    "\n",
    "\"\"\"\n",
    "在 Notebook 里，这一行会回显两个值：\n",
    "dataset 的概况（features、num_rows 等）；\n",
    "第一条样本的字典：{'pixel_values': tensor(...), 'input_ids': tensor([...])}。\n",
    "\"\"\"\n",
    "dataset, dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69854652",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(833,\n",
       " {'pixel_values': tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            ...,\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "  \n",
       "           [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            ...,\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "  \n",
       "           [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            ...,\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.]]]], device='cuda:0'),\n",
       "  'input_ids': tensor([[49406,   320,  3610,   539,   320,  7651,  4009,   530,  3360,   537,\n",
       "            5046, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407]], device='cuda:0')})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义loader\n",
    "def collate_fn(data): # 自定义 打包函数。DataLoader 每次取一个 batch 时，会把该 batch 内的样本列表传进来（data 是一个 list，长度=batch_size；元素是你 dataset[?] 返回的 dict）。\n",
    "    \"\"\"\n",
    "    从这批样本里分别取出图像与文本两列，形成两个 list：\n",
    "    pixel_values：长度 = batch_size，每个元素形状是 [C, 512, 512] 的张量（float32，范围[-1,1]）。\n",
    "    input_ids：长度 = batch_size，每个元素形状是 [77] 的张量（int64）。\n",
    "    \"\"\"\n",
    "    pixel_values = [i['pixel_values'] for i in data]\n",
    "    input_ids = [i['input_ids'] for i in data]\n",
    "\n",
    "    \"\"\"\n",
    "    torch.stack 在新维度上拼接，得到批量张量：\n",
    "    pixel_values 变为 [B, C, 512, 512]\n",
    "    input_ids 变为 [B, 77]\n",
    "    .to(device) 把这两个张量直接搬到设备（'cuda' 或 'cpu'）。\n",
    "    这样做的好处：每个 batch 产出即在目标设备，后面训练/推理时无需再搬运。\n",
    "    \"\"\"\n",
    "    pixel_values = torch.stack(pixel_values).to(device)\n",
    "    input_ids = torch.stack(input_ids).to(device)\n",
    "\n",
    "    # 返回一个批的字典；后续模型前向可直接解包使用。\n",
    "    return {'pixel_values': pixel_values, 'input_ids': input_ids}\n",
    "\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset,\n",
    "                                     shuffle=True,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     batch_size=1)\n",
    "\n",
    "len(loader), next(iter(loader)) # len(loader)：batch 的数量。你的数据集大小之前显示为 833，batch_size=1，因此 len(loader) = 833。\n",
    "# next(iter(loader))：取第一个 batch（因为 shuffle=True，它是随机的一条）。返回字典：\n",
    "# batch['pixel_values']：形状 [1, C, 512, 512]，dtype=torch.float32，位于 device\n",
    "# batch['input_ids']：形状 [1, 77]，dtype=torch.int64，位于 device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53704bed",
   "metadata": {},
   "source": [
    "加载模型 准备训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "217c0a9c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'scaling_factor': 0.18215} were passed to AutoencoderKL, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(AdamW (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     lr: 1e-05\n",
       "     maximize: False\n",
       "     weight_decay: 0.01\n",
       " ),\n",
       " MSELoss())"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#加载模型\n",
    "%run 1.encoder.ipynb\n",
    "%run 2.vae.ipynb\n",
    "%run 3.unet.ipynb\n",
    "\n",
    "#准备训练\n",
    "encoder.requires_grad_(False)\n",
    "vae.requires_grad_(False)\n",
    "unet.requires_grad_(True)\n",
    "\n",
    "encoder.eval()\n",
    "vae.eval()\n",
    "unet.train()\n",
    "\n",
    "encoder.to(device)\n",
    "vae.to(device)\n",
    "unet.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(unet.parameters(),\n",
    "                              lr=1e-5,\n",
    "                              betas=(0.9, 0.999),\n",
    "                              weight_decay=0.01,\n",
    "                              eps=1e-8)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "optimizer, criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355b7e2e",
   "metadata": {},
   "source": [
    "计算 Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d4a806",
   "metadata": {},
   "source": [
    "典型的扩散模型（Stable Diffusion 风格）训练一步：固定文本编码器与VAE，只训练UNet，让它在给定t步的噪声水平下预测噪声 ε，用MSE对齐真噪声。\n",
    "\n",
    "这段代码做了：冻结文本与VAE → 得到条件文本嵌入与潜空间latent → 随机选t把噪声加到latent上形成x_t → 用UNet在(t, 文本条件)下预测噪声ε → 用MSE把预测和真噪声对齐。训练充分后，推理时就能从纯噪声一步步去噪生成潜向量，再经VAE解码回图像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f32e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "定义一个函数做一次前向+计算loss。data 是一个字典，至少包含：\n",
    "data['input_ids']: 文本token ids，形状 [B, 77]（这里示例B=1）\n",
    "data['pixel_values']: 原图像张量，形状 [B, 3, 512, 512]\n",
    "\"\"\"\n",
    "def get_loss(data):\n",
    "    with torch.no_grad(): # 在这个块里关闭梯度。目的：\n",
    "                            # 文本编码器 encoder 和 VAE 编码器通常不训练（冻结），\n",
    "                            # 节省显存和加速前向。\n",
    "        #文字编码\n",
    "        #[1, 77] -> [1, 77, 768]\n",
    "        out_encoder = encoder(data['input_ids'])\n",
    "\n",
    "        \"\"\"\n",
    "        通过VAE把像素空间映射到潜空间（latent）：\n",
    "        vae.encoder(...) 通常输出高斯分布的参数（如均值μ、logvar），形状内部结构由实现决定；\n",
    "        vae.sample(...) 做重参数化采样得到潜向量 z，形状为 [B, 4, 64, 64]。\n",
    "        直觉：512×512 的图片被压到 64×64 的潜空间里，通道数是4。\n",
    "        \"\"\"\n",
    "        #抽取图像特征图\n",
    "        #[1, 3, 512, 512] -> [1, 4, 64, 64]\n",
    "        out_vae = vae.encoder(data['pixel_values'])\n",
    "        out_vae = vae.sample(out_vae)\n",
    "\n",
    "        #0.18215 = vae.config.scaling_factor\n",
    "        out_vae = out_vae * 0.18215 # 关键细节：Stable Diffusion 使用固定缩放因子 0.18215，确保潜空间的数值范围与训练时一致。\n",
    "                                    # 训练和推理两边都要保持这个缩放一致，否则噪声强度/信噪比会错位，导致训练/推理不匹配。\n",
    "\n",
    "    \"\"\"\n",
    "    采样与 latent 同形状的标准高斯噪声 ε：\n",
    "    形状 [B, 4, 64, 64]\n",
    "    这就是训练的监督信号（目标），后面用MSE让UNet去回归它。\n",
    "    \"\"\"\n",
    "    #随机数,unet的计算目标\n",
    "    noise = torch.randn_like(out_vae)\n",
    "\n",
    "    #往特征图中添加噪声\n",
    "    #1000 = scheduler.num_train_timesteps\n",
    "    #1 = batch size\n",
    "    noise_step = torch.randint(0, 1000, (1, )).long().to(device)\n",
    "    \"\"\"\n",
    "    随机选一个时间步 t：\n",
    "    值域 [0, 999]（共1000个训练步，取决于scheduler配置）\n",
    "    形状是 [1]（批量是1）。\n",
    "    如果以后 B>1，通常写成 torch.randint(0, 1000, (B,))，或 [1] 再广播也行，但很多实现要求 [B]，更稳妥。\n",
    "    \"\"\"\n",
    "    out_vae_noise = scheduler.add_noise(out_vae, noise, noise_step)\n",
    "    \"\"\"\n",
    "    把噪声按扩散公式加到latent上，得到 x_t：\n",
    "    数学：x_t = sqrt(ᾱ_t) * x_0 + sqrt(1 - ᾱ_t) * ε\n",
    "    其中 x_0 就是 out_vae（缩放后的latent），ε 是上面采的噪声，ᾱ_t 是scheduler（DDPM/DDIM等）给定的累计噪声计划。\n",
    "    输出形状仍是 [B, 4, 64, 64]。\n",
    "    scheduler.add_noise 内部会查到 t 对应的 sqrt_alpha_cumprod[t] 与 sqrt_one_minus_alpha_cumprod[t] 并做线性组合。\n",
    "    \"\"\"\n",
    "\n",
    "    #根据文字信息,把特征图中的噪声计算出来\n",
    "    out_unet = unet(out_vae=out_vae_noise,\n",
    "                    out_encoder=out_encoder,\n",
    "                    time=noise_step)\n",
    "    \"\"\"\n",
    "    把带噪潜向量 x_t、文本条件、时间步 t 一起喂入UNet：\n",
    "    out_vae_noise: [B, 4, 64, 64]\n",
    "    out_encoder: [B, 77, 768]（做Cross-Attn条件）\n",
    "    time: [B]或[1]（你的UNet里一般会先把t过时间嵌入，如正弦位置编码+MLP → 1280维等）\n",
    "    UNet输出通常是对噪声 ε 的预测：\n",
    "    out_unet 形状 [B, 4, 64, 64]\n",
    "    \"\"\"\n",
    "\n",
    "    #计算mse loss\n",
    "    #[1, 4, 64, 64],[1, 4, 64, 64]\n",
    "    return criterion(out_unet, noise)\n",
    "\n",
    "\n",
    "# get_loss({\n",
    "#     'input_ids': torch.ones(1, 77, device=device).long(),\n",
    "#     'pixel_values': torch.randn(1, 3, 512, 512, device=device)\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ddc16a",
   "metadata": {},
   "source": [
    "一共训练400个epoch，每四个批次我们进行一次参数的调整 \n",
    "\n",
    "它的作用\n",
    "torch.nn.utils.clip_grad_norm_(unet.parameters(), 1.0) =\n",
    "把 unet 所有参数的**梯度向量的整体长度（L2 范数）**限制在 1.0 以内，防止梯度过大（梯度爆炸）。\n",
    "\n",
    "为什么要这么做\n",
    "梯度太大 → 一步更新跳得过猛 → 训练不稳定甚至发散。\n",
    "裁剪后：方向不变，长度变小，更新更稳。\n",
    "\n",
    "它具体干了三步\n",
    "\n",
    "- 把所有参数的梯度拼成一个“大向量”，算它的 L2 范数 total_norm。\n",
    "- 如果 total_norm ≤ 1.0：啥也不做。\n",
    "-  total_norm > 1.0：把所有梯度同时乘以 1.0 / total_norm（统一缩放），让整体范数正好变成 1.0。\n",
    "\n",
    "超简例子\n",
    "假设两个参数的梯度是 g1=3、g2=4，整体范数 sqrt(3^2+4^2)=5 > 1。\n",
    "缩放系数 = 1/5 = 0.2，所以新梯度变为 g1=0.6、g2=0.8。\n",
    "→ 方向相同（比例没变），只是整体“长度”从 5 缩到 1。\n",
    "\n",
    "放在你这段代码里的含义\n",
    "你每累计 4 次 loss.backward() 才 optimizer.step()，这行就在 step 之前把累计后的总梯度裁剪到 ≤ 1.0，保证这次更新不会过猛。\n",
    "\n",
    "两点补充\n",
    "- 这是按范数裁剪（clip_grad_norm_），不同于按数值逐元素截断（clip_grad_value_）。\n",
    "- 函数会原地修改梯度，并返回裁剪前的范数（可用来打印监控）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84b88599",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 11.7118999005761\n",
      "10 105.27776907754014\n",
      "20 101.45478522218764\n",
      "30 97.96161541804031\n",
      "40 95.7652038520173\n",
      "50 92.64628775657911\n",
      "60 91.62508884524868\n",
      "70 88.90302349776903\n",
      "80 84.6358380591555\n",
      "90 82.70271758512536\n",
      "100 81.53195204613439\n",
      "110 76.3927595877758\n",
      "120 74.14106083381193\n",
      "130 71.42537906522921\n",
      "140 69.16221529991162\n",
      "150 65.47076485656726\n",
      "160 62.1360088881047\n",
      "170 60.89056803673884\n",
      "180 57.985315461344726\n",
      "190 54.73302427918679\n",
      "200 50.69724302080431\n",
      "210 48.59712202517403\n",
      "220 46.407517315681616\n",
      "230 44.99496047659704\n",
      "240 44.07751854383969\n",
      "250 39.62402399040002\n",
      "260 37.051896732489695\n",
      "270 36.89249631060375\n",
      "280 35.71413582353853\n",
      "290 33.45783720578038\n",
      "300 33.08240255239798\n",
      "310 30.282505852694158\n",
      "320 29.86848702972202\n",
      "330 29.363934024146147\n",
      "340 29.187604612583527\n",
      "350 27.543819716789585\n",
      "360 26.130621485815936\n",
      "370 25.465440133120865\n",
      "380 25.48384229660587\n",
      "390 24.789676978944044\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    loss_sum = 0\n",
    "    for epoch in range(400):\n",
    "        for i, data in enumerate(loader):\n",
    "            loss = get_loss(data) / 4\n",
    "            loss.backward()\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "            if (epoch * len(loader) + i) % 4 == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(unet.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(epoch, loss_sum)\n",
    "            loss_sum = 0\n",
    "\n",
    "    #torch.save(unet.to('cpu'), 'saves/unet.model')\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22f955b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a72f7737ddf24b868be27b18c5b694a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "711d4f375af84ff59ef05920c646f3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/3.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/lansinuote/diffsion_from_scratch.unet/commit/32f5e4163edb6d1a3fa1d8265ad2cdf0406cb425', commit_message='Upload model', commit_description='', oid='32f5e4163edb6d1a3fa1d8265ad2cdf0406cb425', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "\n",
    "\n",
    "#包装类\n",
    "class Model(PreTrainedModel):\n",
    "    config_class = PretrainedConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.unet = unet.to('cpu')\n",
    "\n",
    "#保存到hub\n",
    "Model(PretrainedConfig()).push_to_hub(\n",
    "    repo_id='lansinuote/diffsion_from_scratch.unet',\n",
    "    use_auth_token=open('/root/hub_token.txt').read().strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
